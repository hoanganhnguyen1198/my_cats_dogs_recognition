{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n!cd data\\n!unzip dogs-vs-cats.zip\\n!unzip train.zip\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "!cd data\n",
    "!unzip dogs-vs-cats.zip\n",
    "!unzip train.zip\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(os.getcwd(), 'data/train/')\n",
    "data_path_cat = data_path + 'cat/'\n",
    "data_path_dog = data_path + 'dog/'\n",
    "os.makedirs(data_path_cat, exist_ok=True)\n",
    "os.makedirs(data_path_dog, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir('data/train/'):\n",
    "    if filename.find('cat.') != -1:\n",
    "        shutil.move(data_path + filename, data_path_cat)\n",
    "    elif filename.find('dog.') != -1:\n",
    "        shutil.move(data_path + filename, data_path_dog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only use 4000 images of cat and 4000 images of dog in total due to memory constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n!cd data/train/cat\\n!find . -type f -print0 | sort -zR | tail -zn +4001 | xargs -0 rm\\n!cd ../dog\\n!find . -type f -print0 | sort -zR | tail -zn +4001 | xargs -0 r\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete some data from the dataset\n",
    "\"\"\"\n",
    "!cd data/train/cat\n",
    "!find . -type f -print0 | sort -zR | tail -zn +4001 | xargs -0 rm\n",
    "!cd ../dog\n",
    "!find . -type f -print0 | sort -zR | tail -zn +4001 | xargs -0 r\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-12 17:57:05.202905: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n",
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate number of samples in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n"
     ]
    }
   ],
   "source": [
    "data_path = pathlib.Path(data_path).with_suffix('')\n",
    "images_count = len(list(data_path.glob('*/*.jpg')))\n",
    "print(images_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "img_height = 200\n",
    "img_width = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset into 3 part:\n",
    "- Test dataset\n",
    "- Train dataset\n",
    "- Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 files belonging to 2 classes.\n",
      "Using 6400 files for training.\n",
      "Using 1600 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = keras.utils.image_dataset_from_directory(\n",
    "    data_path,\n",
    "    validation_split=0.2,\n",
    "    subset='both',\n",
    "    seed=84,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = train_dataset.take(round(train_dataset.cardinality().numpy() * 0.2))\n",
    "train_dataset = train_dataset.skip(round(train_dataset.cardinality().numpy() * 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset batches: tf.Tensor(80, shape=(), dtype=int64)\n",
      "Train dataset batches: tf.Tensor(256, shape=(), dtype=int64)\n",
      "Valid dataset batches: tf.Tensor(64, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(\"Test dataset batches:\", test_dataset.cardinality())\n",
    "print(\"Train dataset batches:\", train_dataset.cardinality())\n",
    "print(\"Valid dataset batches:\", valid_dataset.cardinality())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
